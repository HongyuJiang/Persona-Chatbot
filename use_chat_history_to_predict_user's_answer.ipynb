{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "use chat history to predict user's answer",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HongyuJiang/Dialogue-Generation-with-Persona---A-Note/blob/master/use_chat_history_to_predict_user's_answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "t09eeeR5prIJ"
      },
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "GCCk8_dHpuNf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hcD2nPQvPOFM"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Text generation using a RNN with eager execution\n",
        "\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "<td>\n",
        "<a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/sequences/text_generation\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "</td><td>\n",
        "<a target=\"_blank\"  href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/sequences/text_generation.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>  \n",
        "</td><td>\n",
        "<a target=\"_blank\"  href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/sequences/text_generation.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a></td></table>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "srXC6pLGLwS6"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yG_n40gFzf9s",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "WGyKZj3bzf9p"
      },
      "cell_type": "markdown",
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "metadata": {
        "id": "qilKZW0WLM8D",
        "colab_type": "code",
        "outputId": "9d351b5b-2347-402d-8d55-fc146fcc8c6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8ODIlf2jLZvj",
        "colab_type": "code",
        "outputId": "be130acf-992c-40b5-a1fc-5afc69619198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/dark_learning/chatting_records.txt\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'/content/drive/My Drive/dark_learning/chatting_records.txt'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "EHDoRoc5PKWz"
      },
      "cell_type": "markdown",
      "source": [
        "### Download the Shakespeare dataset\n",
        "\n",
        "Change the following line to run this code on your own data."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "pD_55cOxLkAb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path_to_file = '/content/drive/My Drive/dark_learning/chatting_records2.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OxEiSTwnEXtB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sIKTyotxJnXH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UHjdCjDuSvX_"
      },
      "cell_type": "markdown",
      "source": [
        "### Read the data\n",
        "\n",
        "First, look in the text."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aavnuByVymwK",
        "outputId": "4e21dff7-3e40-48a4-b574-04440929d13a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8').replace('\\ufeff','').replace('[å›¾ç‰‡]', 'å«').replace('[è¡¨æƒ…]', 'å¬²') \n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 2610569 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PNxIz_TBKO0W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lines = text.split('\\r\\n')\n",
        "lines_len = len(lines)\n",
        "name_header = dict()\n",
        "rounds = []\n",
        "\n",
        "content = ''\n",
        "date = ''\n",
        "time = ''\n",
        "speaker = ''\n",
        "\n",
        "for i in range(0, lines_len - 4):\n",
        "    \n",
        "  spaceNum = lines[i].count(' ')\n",
        "  maoNum = lines[i].count(':')\n",
        "\n",
        "  if spaceNum == 2 and maoNum == 2:\n",
        "    \n",
        "    meta = dict()\n",
        "    #print(speaker + ': ' + content + '\\n')\n",
        "    \n",
        "    round_info = lines[i].split(' ')\n",
        "    date = round_info[0]\n",
        "    time = round_info[1]\n",
        "    speaker = round_info[2]\n",
        "    \n",
        "    meta['date'] = date\n",
        "    meta['time'] = time\n",
        "    meta['speaker'] = speaker\n",
        "    meta['content'] = content\n",
        "    \n",
        "    rounds.append(meta)\n",
        "    \n",
        "    content = ''\n",
        "  else:\n",
        "    meta['content'] += lines[i].replace('\\r\\n','')\n",
        "  \n",
        "  if speaker in name_header:\n",
        "    name_header[speaker] += 1\n",
        "  else:\n",
        "    name_header[speaker] = 1\n",
        "  #print(speaker + ': ' + content + ' | ' + date)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8oxpG6Dl-qTG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(0, len(rounds)):\n",
        "  speaker = rounds[i]['speaker']\n",
        "  if speaker in {'è’‹å®å®‡','è’‹æœ‰æ¯’','è’‹æœ‰æ¯’å®¢æœ','æœ‰æ¯’å®¢æœ','ç•¥ç•¥ç•¥','ğŸ·'}:\n",
        "    rounds[i]['speaker'] = 'æˆ‘'\n",
        "  else: \n",
        "    if speaker == 'å•¦':\n",
        "      rounds[i]['speaker'] = 'ä½ '\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5zXtCO1q4jrc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "contex_speaker = 'NULL'\n",
        "summerized_rounds = []\n",
        "\n",
        "\n",
        "for i in range(0, len(rounds)):\n",
        "  speaker = rounds[i]['speaker']\n",
        "  if speaker == contex_speaker:\n",
        "    context_meta['content'] += ' ' + rounds[i]['content']\n",
        "  else:\n",
        "    if contex_speaker != 'NULL':\n",
        "      summerized_rounds.append(context_meta)\n",
        "      \n",
        "    contex_speaker = speaker\n",
        "    context_meta = dict()\n",
        "    context_meta['date'] = rounds[i]['date']\n",
        "    context_meta['time'] = rounds[i]['time']\n",
        "    context_meta['speaker'] = rounds[i]['speaker']\n",
        "    context_meta['content'] = rounds[i]['content']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2yxsrrMyECd8",
        "colab_type": "code",
        "outputId": "3efedeac-2c30-482d-f74b-ba9adaba2b22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(summerized_rounds))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KvElLdwNAEhd",
        "colab_type": "code",
        "outputId": "2405b84a-b0ca-4b95-ae0d-ab77b2a24fb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install colored\n",
        "import colored\n",
        "from colored import stylize\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: colored in /usr/local/lib/python3.6/dist-packages (1.3.93)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SW3RJuPEDY6w",
        "colab_type": "code",
        "outputId": "686c6f0b-f316-4d14-d690-41c1e326bf1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "Lan = colored.fg(\"white\") + colored.bg(\"blue\")\n",
        "\n",
        "Yu = colored.fg(\"white\") + colored.bg(\"red\")\n",
        "\n",
        "for i in range(19950,19960):\n",
        "  if i % 2 == 0:\n",
        "    print(stylize(summerized_rounds[i], Yu))\n",
        "  else:\n",
        "    print(stylize(summerized_rounds[i], Lan))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;15m\u001b[48;5;1m{'date': '2019-01-04', 'time': '0:57:39', 'speaker': 'ä½ ', 'content': 'æˆ‘ç»™ä½ é¼“æŒ ç„¶å ç»™ä½ huo'}\u001b[0m\n",
            "\u001b[38;5;15m\u001b[48;5;4m{'date': '2019-01-04', 'time': '0:57:56', 'speaker': 'æˆ‘', 'content': 'å¦ˆå–æ‰¹ ä½ ä¸ªç“œå¨ƒå­ ä½ è¦é­'}\u001b[0m\n",
            "\u001b[38;5;15m\u001b[48;5;1m{'date': '2019-01-04', 'time': '0:58:09', 'speaker': 'ä½ ', 'content': 'å«'}\u001b[0m\n",
            "\u001b[38;5;15m\u001b[48;5;4m{'date': '2019-01-04', 'time': '0:58:10', 'speaker': 'æˆ‘', 'content': 'ç“œæ‰¹äº‘'}\u001b[0m\n",
            "\u001b[38;5;15m\u001b[48;5;1m{'date': '2019-01-04', 'time': '0:58:22', 'speaker': 'ä½ ', 'content': 'ä½ è¯´çš„æ‹¿æ¯å­çš„å˜› æˆ‘ä»¥ä¸ºä½ æƒ³huo'}\u001b[0m\n",
            "\u001b[38;5;15m\u001b[48;5;4m{'date': '2019-01-04', 'time': '0:58:36', 'speaker': 'æˆ‘', 'content': 'ç»™ä½ å–çš„'}\u001b[0m\n",
            "\u001b[38;5;15m\u001b[48;5;1m{'date': '2019-01-04', 'time': '0:58:42', 'speaker': 'ä½ ', 'content': 'æ—¢ç„¶ä½ ä¸æƒ³å–'}\u001b[0m\n",
            "\u001b[38;5;15m\u001b[48;5;4m{'date': '2019-01-04', 'time': '0:58:45', 'speaker': 'æˆ‘', 'content': 'å¼ å…°äº‘ä¸€é¥®è€Œå°½'}\u001b[0m\n",
            "\u001b[38;5;15m\u001b[48;5;1m{'date': '2019-01-04', 'time': '0:58:48', 'speaker': 'ä½ ', 'content': 'é‚£ä½ æ‹¿å»ä¸‹é¢'}\u001b[0m\n",
            "\u001b[38;5;15m\u001b[48;5;4m{'date': '2019-01-04', 'time': '0:58:55', 'speaker': 'æˆ‘', 'content': 'å˜æ€ï¼ æ­»å˜æ€ï¼'}\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IlCgQBRVymwR",
        "outputId": "b19546df-1cba-4286-eba8-0e6cce8a5dfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3339 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rNnrKn_lL-IJ"
      },
      "cell_type": "markdown",
      "source": [
        "## Process the text"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "LFjSVAlWzf-N"
      },
      "cell_type": "markdown",
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before training, we need to map strings to a numerical representation. Create two lookup tables: one mapping characters to numbers, and another for numbers to characters."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IalZLbvOzf-F",
        "outputId": "7b2b428d-cfb4-4ad2-be1f-3331aab0c21b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "#char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "\n",
        "char2idx = dict()\n",
        "idx2char = dict()\n",
        "\n",
        "idx2char[1] = 'EOF'\n",
        "\n",
        "for i,u in enumerate(vocab):\n",
        "  char2idx[u] = i + 2\n",
        "  idx2char[i + 2] = u\n",
        "\n",
        "for i in range(len(summerized_rounds)):\n",
        "  \n",
        "  if len(summerized_rounds[i]['content']) > 50:\n",
        "    summerized_rounds[i]['content'] = ''\n",
        "  temp_str = ''\n",
        "  for j,u in enumerate(summerized_rounds[i]['content']):\n",
        "    temp_str += ' ' + str(char2idx[u])\n",
        "  summerized_rounds[i]['content_intCode'] = temp_str\n",
        "  #summerized_rounds[i]['content'] = '' #clear cache\n",
        "  \n",
        "print(len(summerized_rounds))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "de4p5OltlynO",
        "colab_type": "code",
        "outputId": "006dd169-b6fb-439f-c003-5bc752b18437",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  print(summerized_rounds[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'date': '2018-09-08', 'time': '8:20:40', 'speaker': 'ä½ ', 'content': 'å¸ˆå…„ ä½ ä»€ä¹ˆæ–¹å‘å•Š?', 'content_intCode': ' 1058 344 5 281 219 173 1502 575 675 36'}\n",
            "{'date': '2018-09-08', 'time': '8:21:02', 'speaker': 'æˆ‘', 'content': 'å¯è§†åŒ–æ–¹å‘ æ–°ç”Ÿè§é¢ä¼šä½ å¥½åƒæ²¡æ¥å§', 'content_intCode': ' 556 2736 478 1502 575 5 1501 2031 2732 3179 255 281 859 337 1761 1593 584'}\n",
            "{'date': '2018-09-08', 'time': '8:50:28', 'speaker': 'ä½ ', 'content': 'ç§‘å­¦å¯è§†åŒ–è·Ÿä¿¡æ¯å¯è§†åŒ–éƒ½æ˜¯å—? å«', 'content_intCode': ' 2214 930 556 2736 478 2894 308 1201 556 2736 478 3031 1528 578 36 5 912'}\n",
            "{'date': '2018-09-08', 'time': '11:01:50', 'speaker': 'æˆ‘', 'content': 'éƒ½åšè¿‡ç‚¹ ä¸»è¦è¿˜æ˜¯åšå¯è§†åˆ†æ å«', 'content_intCode': ' 3031 325 2956 1908 5 168 2729 2962 1528 325 556 2736 414 1602 5 912'}\n",
            "{'date': '2018-09-08', 'time': '12:00:17', 'speaker': 'ä½ ', 'content': 'å¸ˆå…„æˆ‘ä»¥åå¯èƒ½æ˜¯åšç§»åŠ¨é€šä¿¡æ–¹é¢çš„ï¼Œè¿™è¾¹å®éªŒå®¤çš„å¸ˆå…„è¯´æˆ‘å­¦çš„è·Ÿä½ æ¯”è¾ƒåƒï¼Œå¸ˆå…„ä»¥åå¤šæŒ‡æ•™ å«', 'content_intCode': ' 1058 344 1272 236 573 556 2477 1528 325 2223 460 2987 308 1502 3179 2091 3325 2963 2951 949 3254 954 2091 1058 344 2797 1272 930 2091 2894 281 1726 2933 337 3325 1058 344 236 573 829 1371 1477 5 912'}\n",
            "{'date': '2018-09-08', 'time': '12:00:48', 'speaker': 'æˆ‘', 'content': 'ä½ åšè”é€šæ•°æ®å•Š å¯ä»¥çš„å‘¢ å‰æœŸæˆ‘ä»¬å·²ç»å‘è¿‡å‡ ç¯‡æ–‡ç« äº†', 'content_intCode': ' 281 325 2438 2987 1484 1399 675 5 556 236 2091 608 5 443 1569 1272 238 1049 2356 538 2956 400 2290 1488 2255 194'}\n",
            "{'date': '2018-09-08', 'time': '12:01:21', 'speaker': 'ä½ ', 'content': 'å…³é”®æˆ‘ç°åœ¨å°±æ˜¯ä¸ªèœé¸Ÿ æˆ‘ä»Šå¤©è¿˜çœ‹äº†ä½ çš„è®ºæ–‡', 'content_intCode': ' 366 3094 1272 1998 764 998 1528 160 2601 3283 5 1272 224 833 2962 2117 194 281 2091 2768 1488'}\n",
            "{'date': '2018-09-08', 'time': '12:01:39', 'speaker': 'æˆ‘', 'content': 'å¯ä»¥ä¸Šå®éªŒå®¤ä¸»é¡µçœ‹çœ‹ï¼Œåˆä¸èƒ½å¯å‘ä¸€ä¸‹ æ‰¾æ‰¾æ–°çš„ç‚¹', 'content_intCode': ' 556 236 143 949 3254 954 168 3190 2117 2117 3325 532 145 2477 589 538 137 144 5 1308 1308 1501 2091 1908'}\n",
            "{'date': '2018-09-08', 'time': '12:01:48', 'speaker': 'ä½ ', 'content': 'å¥½ç‰›é€¼çš„æ ·å­', 'content_intCode': ' 859 1955 2997 2091 1632 919'}\n",
            "{'date': '2018-09-08', 'time': '12:01:53', 'speaker': 'æˆ‘', 'content': 'å«', 'content_intCode': ' 912'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "bbmsf23Bymwe"
      },
      "cell_type": "markdown",
      "source": [
        "### The prediction task"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0UHJDA39zf-O",
        "outputId": "7bda191d-a07e-4cf7-e807-4daa1435435e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "PAD = 0\n",
        "EOS = 1\n",
        "\n",
        "training_data = []\n",
        "\n",
        "for i in range(0, len(summerized_rounds)-1, 2):\n",
        "  meta = dict()\n",
        "  inp = summerized_rounds[i]['content']\n",
        "  out = summerized_rounds[i + 1]['content']\n",
        "  code_input = summerized_rounds[i]['content_intCode'].split(' ')\n",
        "  code_output = summerized_rounds[i+1]['content_intCode'].split(' ')\n",
        "  meta['input'] = code_input[1:-1]\n",
        "  meta['output'] = code_output[1:-1]\n",
        "  \n",
        "  inp_emojis_count = inp.count('å¬²')\n",
        "  out_emojis_count = out.count('å¬²')\n",
        "  \n",
        "  inp_images_count = inp.count('å«')\n",
        "  out_images_count = out.count('å«')\n",
        "  \n",
        "  inp_modal_count = inp.count('å“ˆ')\n",
        "  out_modal_count = out.count('å“ˆ')\n",
        "  \n",
        "  inp_space_count = inp.count(' ')\n",
        "  out_space_count = out.count(' ')\n",
        "  \n",
        "  inp_useless_count = inp_emojis_count + inp_images_count + inp_space_count + inp_modal_count\n",
        "  out_useless_count = out_emojis_count + out_images_count + out_space_count + out_modal_count\n",
        "  \n",
        "  if inp_useless_count < len(inp) and out_useless_count < len(out) and len(inp) > 0 and len(out) > 0:\n",
        "    training_data.append(meta)\n",
        "  #summerized_rounds[i]['content'] = ''\n",
        "  #summerized_rounds[i+1]['content'] = ''\n",
        "  \n",
        "for i in range(len(training_data)):\n",
        "  for j in range(len(training_data[i]['input'])):\n",
        "    training_data[i]['input'][j] = int(training_data[i]['input'][j])\n",
        "  for j in range(len(training_data[i]['output'])):\n",
        "    training_data[i]['output'][j] = int(training_data[i]['output'][j])\n",
        "    \n",
        "    \n",
        "print(len(training_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QdcS9gBVqHNC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "input_embedding_size = 128\n",
        "\n",
        "encoder_hidden_units = 20\n",
        "decoder_hidden_units = encoder_hidden_units * 2\n",
        "\n",
        "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
        "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
        "decoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='decoder_inputs_length')\n",
        "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-ZSYAcQV8OGP"
      },
      "cell_type": "markdown",
      "source": [
        "The `batch` method lets us easily convert these individual characters to sequences of the desired size. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "l4hkDU3i7ozi",
        "outputId": "0fb86de8-d4f9-4ccf-994c-00ec73a16e59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "cell_type": "code",
      "source": [
        "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
        "\n",
        "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UbLcIPBj_mWZ"
      },
      "cell_type": "markdown",
      "source": [
        "For each sequence, duplicate and shift it to form the input and target text by using the `map` method to apply a simple function to each batch:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "9NGu-FkO_kYU",
        "outputId": "79e01e9f-74c3-4ab1-f898-c7634950dfa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple\n",
        "\n",
        "encoder_cell = LSTMCell(encoder_hidden_units)\n",
        "\n",
        "((encoder_fw_outputs,\n",
        "  encoder_bw_outputs),\n",
        " (encoder_fw_final_state,\n",
        "  encoder_bw_final_state)) = (\n",
        "    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
        "                                    cell_bw=encoder_cell,\n",
        "                                    inputs=encoder_inputs_embedded,\n",
        "                                    sequence_length=encoder_inputs_length,\n",
        "                                    dtype=tf.float32, time_major=True)\n",
        "    )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-19-47050786f5d5>:3: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-19-47050786f5d5>:13: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xz2PRf4tJ3th",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
        "\n",
        "encoder_final_state_c = tf.concat(\n",
        "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
        "\n",
        "encoder_final_state_h = tf.concat(\n",
        "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
        "\n",
        "encoder_final_state = LSTMStateTuple(\n",
        "    c=encoder_final_state_c,\n",
        "    h=encoder_final_state_h\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hiCopyGZymwi"
      },
      "cell_type": "markdown",
      "source": [
        "Print the first examples input and target values:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GNbw-iR0ymwj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_cell = LSTMCell(decoder_hidden_units)\n",
        "\n",
        "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))\n",
        "\n",
        "decoder_lengths = decoder_inputs_length #å¿…é¡»åŠ ä¸ŠPADå’ŒEOS Tensoræ‰ä¼šåŒ¹é…\n",
        "#decoder_lengths = encoder_inputs_length\n",
        "\n",
        "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32)\n",
        "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2YPKtfOdKFLP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "assert EOS == 1 and PAD == 0\n",
        "\n",
        "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
        "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
        "\n",
        "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
        "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UkA2vEHlKH-e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loop_fn_initial():\n",
        "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
        "    initial_input = eos_step_embedded\n",
        "    initial_cell_state = encoder_final_state\n",
        "    initial_cell_output = None\n",
        "    initial_loop_state = None  # we don't need to pass any additional information\n",
        "    return (initial_elements_finished,\n",
        "            initial_input,\n",
        "            initial_cell_state,\n",
        "            initial_cell_output,\n",
        "            initial_loop_state)\n",
        "  \n",
        "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
        "\n",
        "    def get_next_input():\n",
        "        output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
        "        prediction = tf.argmax(output_logits, axis=1)\n",
        "        next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
        "        return next_input\n",
        "    \n",
        "    elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n",
        "                                                  # defining if corresponding sequence has ended\n",
        "\n",
        "    finished = tf.reduce_all(elements_finished) # -> boolean scalar\n",
        "    input = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
        "    state = previous_state\n",
        "    output = previous_output\n",
        "    loop_state = None\n",
        "\n",
        "    return (elements_finished, \n",
        "            input,\n",
        "            state,\n",
        "            output,\n",
        "            loop_state)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JeSQDH9NKjGP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
        "    if previous_state is None:    # time == 0\n",
        "        assert previous_output is None and previous_state is None\n",
        "        return loop_fn_initial()\n",
        "    else:\n",
        "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
        "\n",
        "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
        "decoder_outputs = decoder_outputs_ta.stack()\n",
        "\n",
        "\n",
        "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
        "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
        "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
        "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))\n",
        "\n",
        "decoder_prediction = tf.argmax(decoder_logits, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_33OHL3b84i0"
      },
      "cell_type": "markdown",
      "source": [
        "Each index of these vectors are processed as one time step. For the input at time step 0, the model receives the index for \"F\" and trys to predict the index for \"i\" as the next character. At the next timestep, it does the same thing but the `RNN` considers the previous step context in addition to the current input character."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0eBu9WZG84i0",
        "outputId": "369c1fc8-b0ec-4e17-ccc2-7bceb9635d4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "cell_type": "code",
      "source": [
        "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
        "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
        "    logits=decoder_logits,\n",
        ")\n",
        "\n",
        "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
        "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-25-2162ef0beb6a>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MJdfPmdqzf-R"
      },
      "cell_type": "markdown",
      "source": [
        "### Create training batches\n",
        "\n",
        "We used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, we need to shuffle the data and pack it into batches."
      ]
    },
    {
      "metadata": {
        "id": "tJU1dMhQO0W-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batchFun(tranining_data, max_sequence_length=None):\n",
        "    \n",
        "    inputs_lengths = [(len(seq['input']) + 1) for seq in tranining_data]\n",
        "    outputs_lengths = [(len(seq['output']) + 1) for seq in tranining_data]\n",
        "    \n",
        "    batch_size = len(tranining_data)\n",
        "    \n",
        "    max_lens = 0\n",
        "    \n",
        "    #if max_sequence_length is None:\n",
        "    max_inputs_length = max(inputs_lengths)\n",
        "    max_outputs_length = max(outputs_lengths)\n",
        "    \n",
        "    #print(max_inputs_length, max_outputs_length)\n",
        "        \n",
        "    #print(batch_size, max_inputs_length, max_outputs_length)\n",
        "  \n",
        "    if max_inputs_length > max_outputs_length:\n",
        "      max_lens = max_inputs_length\n",
        "    else:\n",
        "      max_lens = max_outputs_length\n",
        "    \n",
        "    inputs_batch_major = np.zeros(shape=[batch_size, max_inputs_length], dtype=np.int32) # == PAD\n",
        "    \n",
        "    outputs_batch_major = np.zeros(shape=[batch_size, max_outputs_length], dtype=np.int32) # == PAD\n",
        "    \n",
        "    \n",
        "    #ç»™å›ç­”å¢åŠ PADå’ŒEOS\n",
        "    \n",
        "    for i, seq in enumerate(tranining_data):\n",
        "        for j, element in enumerate(seq['input']):\n",
        "            inputs_batch_major[i, j] = element\n",
        "        inputs_batch_major[i, max_inputs_length - 1]  =  EOS\n",
        "            \n",
        "    for i, seq in enumerate(tranining_data):\n",
        "        for j, element in enumerate(seq['output']):\n",
        "            outputs_batch_major[i, j] = element\n",
        "        outputs_batch_major[i, max_outputs_length - 1]  =  EOS\n",
        "     \n",
        "    # [batch_size, max_time] -> [max_time, batch_size]\n",
        "    inputs_time_major = inputs_batch_major.swapaxes(0, 1)\n",
        "    outputs_time_major = outputs_batch_major.swapaxes(0, 1)\n",
        "\n",
        "    return inputs_time_major, outputs_time_major, inputs_lengths, outputs_lengths\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "r6oUuElIMgVx"
      },
      "cell_type": "markdown",
      "source": [
        "## Build The Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "m8gPwEjRzf-Z"
      },
      "cell_type": "markdown",
      "source": [
        "Use `tf.keras.Sequential` to define the model. For this simple example three layers are used to define our model:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions;\n",
        "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use a LSTM layer here.)\n",
        "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zHT8cLh7EAsg",
        "outputId": "9c6714ae-c784-4aed-e51b-443e0ae1bca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "print(len(training_data))\n",
        "\n",
        "def next_feed():\n",
        "    global startNum\n",
        "    global endNum\n",
        "  \n",
        "    _batch = training_data[10000: 11000]\n",
        "    encoder_inputs_, decoder_targets_, encoder_input_lengths_, encoder_output_lengths_ = batchFun(_batch)\n",
        "    \n",
        "    #print(len(encoder_inputs_[0]), len(decoder_targets_[0]))\n",
        "    \n",
        "    if endNum < len(training_data):\n",
        "      startNum += 1000\n",
        "      endNum += 1000\n",
        "    else:\n",
        "      startNum = 0\n",
        "      endNum = 1000\n",
        "\n",
        "    return {\n",
        "        encoder_inputs: encoder_inputs_,\n",
        "        encoder_inputs_length: encoder_input_lengths_,\n",
        "        decoder_targets: decoder_targets_,\n",
        "        decoder_inputs_length: encoder_output_lengths_,\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "357DDTkwCXeD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NNVB-jmMEOzP"
      },
      "cell_type": "markdown",
      "source": [
        "Next define a function to build the model.\n",
        "\n",
        "Use `CuDNNGRU` if running on GPU.  "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MtCrdfzEI2N0",
        "outputId": "ff11764e-5291-40fc-d144-118dab2ba951",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2663
        }
      },
      "cell_type": "code",
      "source": [
        "loss_track = []\n",
        "\n",
        "max_batches = 3001\n",
        "batches_in_epoch = 500\n",
        "\n",
        "startNum = 0\n",
        "endNum = 1000\n",
        "\n",
        "try:\n",
        "    for batch in range(max_batches):\n",
        "        fd = next_feed()\n",
        "        _, l = sess.run([train_op, loss], fd)\n",
        "        loss_track.append(l)\n",
        "\n",
        "        if batch % batches_in_epoch == 0:\n",
        "            print('batch {} ---------------------------------------'.format(batch))\n",
        "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
        "            predict_ = sess.run(decoder_prediction, fd)\n",
        "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
        "              print('sample {}:'.format(i + 1))\n",
        "              myStr = ''\n",
        "              for j in range(len(inp)):\n",
        "                if inp[j] != 0 and inp[j] != 1:\n",
        "                  myStr += idx2char[inp[j]]\n",
        "              print('Q >:{}|'.format(myStr))\n",
        "              myStr = ''\n",
        "              for j in range(len(pred)):\n",
        "                if pred[j] != 0 and pred[j] != 1:\n",
        "                  myStr += idx2char[pred[j]]\n",
        "              print('A >:{}|'.format(myStr))\n",
        "              if i >= 5:\n",
        "                  break\n",
        "            print('-----------------------------------------------------')\n",
        "            \n",
        "except KeyboardInterrupt:\n",
        "    print('training interrupted')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch 0 ---------------------------------------\n",
            "  minibatch loss: 8.151241302490234\n",
            "sample 1:\n",
            "Q >:æ‰€ä»¥äº‹æƒ…å°±è¯´å¾—é€š|\n",
            "A >:æ’­å¾Šè¾—è€¶é¥²è™•è™•æˆæˆ|\n",
            "sample 2:\n",
            "Q >:æˆ‘ä¹Ÿè§‰å¾—ä½ æŒºä¼˜ç§€çš„ ä¸è¿‡ä½ ä¹Ÿä¸æƒ³è®²ç©¶ å°†å°± æ¯•ç«Ÿé‡åˆ°ä¸€ä¸ªå¾ˆåˆé€‚çš„äººæ¦‚ç‡æ¯”è¾ƒ|\n",
            "A >:å—¦æ ‹bèœ·æ¯…èœ·éš†è™šç­·ç­·åé¥®|\n",
            "sample 3:\n",
            "Q >:æˆ‘è§‰å¾—è¿˜æ˜¯æˆ‘ä»¬ä¸¤ä¸ªåœ¨ä¸€èµ·æ¯”è¾ƒ|\n",
            "A >:å‡°å‡°æ’µå’¸æŒºä¸å®´æ€å•¦èƒåºœæœ‹ç„–æ€•æ©Ÿå¤•ç½©é›€é›€æ·¹|\n",
            "sample 4:\n",
            "Q >:æˆ‘ä¸ä¼šä¸ºäº†èº«ä½“çš„æ„‰æ‚¦ å»èˆå¼ƒç²¾ç¥çš„æ„‰æ‚¦|\n",
            "A >:å‡°å‡°æ’µå’¸æŒºä¸å®´è´¯ä¿ƒè‚è…Œä¼ªè·µèœ‚é›²åµŒåµŒç£¨æ°å°æ·±è€³å¯’å€¦|\n",
            "sample 5:\n",
            "Q >:ç¡®å®ä¸æ’é™¤è¿™ç§å¯èƒ½æ€§ é‚£å°±éœ€è¦è´£ä»»å¿ƒå»å¹²æ¶‰|\n",
            "A >:è°æˆ’é©±é”€éç¬‘ç«¯ç®•ç™¾å¡‘è¾ƒé€å¼¯èš€æ”€å’å›å–±æ´»|\n",
            "sample 6:\n",
            "Q >:æ˜¯çš„|\n",
            "è§¦å“¦æ ‹æ¥¼å§¿æƒ§å§¿æ‰“ï¿¢æ˜Šæ›¸å®ƒé…Œ|\n",
            "-----------------------------------------------------\n",
            "batch 500 ---------------------------------------\n",
            "  minibatch loss: 6.323128700256348\n",
            "sample 1:\n",
            "Q >:æ‰€ä»¥äº‹æƒ…å°±è¯´å¾—é€š|\n",
            "A >:æˆ‘æˆ‘   |\n",
            "sample 2:\n",
            "Q >:æˆ‘ä¹Ÿè§‰å¾—ä½ æŒºä¼˜ç§€çš„ ä¸è¿‡ä½ ä¹Ÿä¸æƒ³è®²ç©¶ å°†å°± æ¯•ç«Ÿé‡åˆ°ä¸€ä¸ªå¾ˆåˆé€‚çš„äººæ¦‚ç‡æ¯”è¾ƒ|\n",
            "A >:å¯¹å¯¹ç»™ |\n",
            "sample 3:\n",
            "Q >:æˆ‘è§‰å¾—è¿˜æ˜¯æˆ‘ä»¬ä¸¤ä¸ªåœ¨ä¸€èµ·æ¯”è¾ƒ|\n",
            "A >:ä½ ä½ ä½    å“ˆ å“ˆ  å“ˆ   å“ˆ  |\n",
            "sample 4:\n",
            "Q >:æˆ‘ä¸ä¼šä¸ºäº†èº«ä½“çš„æ„‰æ‚¦ å»èˆå¼ƒç²¾ç¥çš„æ„‰æ‚¦|\n",
            "A >:å“ˆå“ˆå“ˆ               |\n",
            "sample 5:\n",
            "Q >:ç¡®å®ä¸æ’é™¤è¿™ç§å¯èƒ½æ€§ é‚£å°±éœ€è¦è´£ä»»å¿ƒå»å¹²æ¶‰|\n",
            "A >:è¿™è¦ä¸è¯´çš„çš„ ä¸ä¸ä¸ä¸ä¸çš„çš„ ä¸|\n",
            "sample 6:\n",
            "Q >:æ˜¯çš„|\n",
            "A >:æˆ‘å“ˆ  å“ˆ  |\n",
            "-----------------------------------------------------\n",
            "batch 1000 ---------------------------------------\n",
            "  minibatch loss: 5.362196922302246\n",
            "sample 1:\n",
            "Q >:æ‰€ä»¥äº‹æƒ…å°±è¯´å¾—é€š|\n",
            "A >:æˆ‘æˆ‘   å¦ˆ|\n",
            "sample 2:\n",
            "Q >:æˆ‘ä¹Ÿè§‰å¾—ä½ æŒºä¼˜ç§€çš„ ä¸è¿‡ä½ ä¹Ÿä¸æƒ³è®²ç©¶ å°†å°± æ¯•ç«Ÿé‡åˆ°ä¸€ä¸ªå¾ˆåˆé€‚çš„äººæ¦‚ç‡æ¯”è¾ƒ|\n",
            "A >:å¯¹å•Š  æˆ‘ æˆ‘ ä¸ª|\n",
            "sample 3:\n",
            "Q >:æˆ‘è§‰å¾—è¿˜æ˜¯æˆ‘ä»¬ä¸¤ä¸ªåœ¨ä¸€èµ·æ¯”è¾ƒ|\n",
            "A >:ä½ æƒ³ä½    å“ˆ å“ˆå“ˆ å“ˆ å“ˆå“ˆå“ˆå“ˆ|\n",
            "sample 4:\n",
            "Q >:æˆ‘ä¸ä¼šä¸ºäº†èº«ä½“çš„æ„‰æ‚¦ å»èˆå¼ƒç²¾ç¥çš„æ„‰æ‚¦|\n",
            "A >:æˆ‘æ—¥çš„çš„     å“ˆ ä½  ä½  æˆ‘æˆ‘ ä½  ä½   æˆ‘|\n",
            "sample 5:\n",
            "Q >:ç¡®å®ä¸æ’é™¤è¿™ç§å¯èƒ½æ€§ é‚£å°±éœ€è¦è´£ä»»å¿ƒå»å¹²æ¶‰|\n",
            "A >:è¿™ä¸ªä¸ªä¸ä¸çš„ä¸ä¸ä¸ä¸ä¸çš„ä¸ä¸ä¸ä¸ä¸ä¸ä¸|\n",
            "sample 6:\n",
            "Q >:æ˜¯çš„|\n",
            "A >:æ‚¨æˆ‘å“ˆå“ˆå“ˆå“ˆå“ˆå“ˆçš„çš„çš„çš„çš„    |\n",
            "-----------------------------------------------------\n",
            "batch 1500 ---------------------------------------\n",
            "  minibatch loss: 4.488375186920166\n",
            "sample 1:\n",
            "Q >:æ‰€ä»¥äº‹æƒ…å°±è¯´å¾—é€š|\n",
            "A >:æˆ‘å» æˆ‘å¦ˆå¦ˆçš„|\n",
            "sample 2:\n",
            "Q >:æˆ‘ä¹Ÿè§‰å¾—ä½ æŒºä¼˜ç§€çš„ ä¸è¿‡ä½ ä¹Ÿä¸æƒ³è®²ç©¶ å°†å°± æ¯•ç«Ÿé‡åˆ°ä¸€ä¸ªå¾ˆåˆé€‚çš„äººæ¦‚ç‡æ¯”è¾ƒ|\n",
            "A >:å¯¹å•Š ä½ æ‰€å¥¹æˆ‘æˆ‘å‡†å‡† å¤‡|\n",
            "sample 3:\n",
            "Q >:æˆ‘è§‰å¾—è¿˜æ˜¯æˆ‘ä»¬ä¸¤ä¸ªåœ¨ä¸€èµ·æ¯”è¾ƒ|\n",
            "A >:ä½ æƒ³æƒ³æ¸…  å“ˆå“ˆ å“ˆå“ˆ å“ˆå“ˆ å“ˆå“ˆå“ˆ|\n",
            "sample 4:\n",
            "Q >:æˆ‘ä¸ä¼šä¸ºäº†èº«ä½“çš„æ„‰æ‚¦ å»èˆå¼ƒç²¾ç¥çš„æ„‰æ‚¦|\n",
            "A >:ä¸‡ä¸€ä¸€å‘çš„    æ¯”è¶£æ¯”   æˆ‘æˆ‘|\n",
            "sample 5:\n",
            "Q >:ç¡®å®ä¸æ’é™¤è¿™ç§å¯èƒ½æ€§ é‚£å°±éœ€è¦è´£ä»»å¿ƒå»å¹²æ¶‰|\n",
            "A >:è¿™ä¸ªä¸ªä¹Ÿä¸è¯´è¯´ç°çš„ä¸ä¸ä¸ä¸ä¸ä¸ä¸ä¸|\n",
            "sample 6:\n",
            "Q >:æ˜¯çš„|\n",
            "A >:æ‚¨ä»Šå¯æ˜¯ä¸Šæ˜¯ä¸Šæ˜¯çš„ä¼šä¸ |\n",
            "-----------------------------------------------------\n",
            "batch 2000 ---------------------------------------\n",
            "  minibatch loss: 3.6685562133789062\n",
            "sample 1:\n",
            "Q >:æ‰€ä»¥äº‹æƒ…å°±è¯´å¾—é€š|\n",
            "A >:æˆ‘å» æˆ‘å¦ˆå¦ˆçš„|\n",
            "sample 2:\n",
            "Q >:æˆ‘ä¹Ÿè§‰å¾—ä½ æŒºä¼˜ç§€çš„ ä¸è¿‡ä½ ä¹Ÿä¸æƒ³è®²ç©¶ å°†å°± æ¯•ç«Ÿé‡åˆ°ä¸€ä¸ªå¾ˆåˆé€‚çš„äººæ¦‚ç‡æ¯”è¾ƒ|\n",
            "A >:å¯¹å•Š æ‰€ä»¥æˆ‘å¦¹æˆ‘å‡†å‡†|\n",
            "sample 3:\n",
            "Q >:æˆ‘è§‰å¾—è¿˜æ˜¯æˆ‘ä»¬ä¸¤ä¸ªåœ¨ä¸€èµ·æ¯”è¾ƒ|\n",
            "A >:ä½ æƒ³æƒ³æ¸…æ¥šï¼Œ å“ˆå“ˆå“ˆ å“ˆ å“ˆå“ˆ å“ˆ|\n",
            "sample 4:\n",
            "Q >:æˆ‘ä¸ä¼šä¸ºäº†èº«ä½“çš„æ„‰æ‚¦ å»èˆå¼ƒç²¾ç¥çš„æ„‰æ‚¦|\n",
            "A >:ä¸‡ä¸€ä½ å‘çš„  æ¯”æ¯”æ¯”   æˆ‘æˆ‘æˆ‘ä¸ªå¿«|\n",
            "sample 5:\n",
            "Q >:ç¡®å®ä¸æ’é™¤è¿™ç§å¯èƒ½æ€§ é‚£å°±éœ€è¦è´£ä»»å¿ƒå»å¹²æ¶‰|\n",
            "A >:è¿™ä¸ªä¹Ÿä¸ä»¬è¯´ç°åœ¨çš„ä¸ä¸ä¸ä¸ä¸ä¸ä¸ä¸ä¸|\n",
            "sample 6:\n",
            "Q >:æ˜¯çš„|\n",
            "A >:æ‚¨ä»Šå¤©ä¸Šæ˜¯å¯ä¸Šæ˜¯å¯å“ˆå“ˆå“ˆ|\n",
            "-----------------------------------------------------\n",
            "batch 2500 ---------------------------------------\n",
            "  minibatch loss: 2.8950717449188232\n",
            "sample 1:\n",
            "Q >:æ‰€ä»¥äº‹æƒ…å°±è¯´å¾—é€š|\n",
            "A >:æˆ‘å» æˆ‘å¦ˆå¦ˆåœ¨è¯´|\n",
            "sample 2:\n",
            "Q >:æˆ‘ä¹Ÿè§‰å¾—ä½ æŒºä¼˜ç§€çš„ ä¸è¿‡ä½ ä¹Ÿä¸æƒ³è®²ç©¶ å°†å°± æ¯•ç«Ÿé‡åˆ°ä¸€ä¸ªå¾ˆåˆé€‚çš„äººæ¦‚ç‡æ¯”è¾ƒ|\n",
            "A >:å¯¹å•Š æ‰€ä»¥ä½ è¦ç¡¬å‡†å¤‡å˜|\n",
            "sample 3:\n",
            "Q >:æˆ‘è§‰å¾—è¿˜æ˜¯æˆ‘ä»¬ä¸¤ä¸ªåœ¨ä¸€èµ·æ¯”è¾ƒ|\n",
            "A >:ä½ æƒ³æƒ³æ¸…æ¥šå“¦ å“ˆå“ˆ å“ˆ å¥½å“ˆå“ˆ å¥½å“ˆ|\n",
            "sample 4:\n",
            "Q >:æˆ‘ä¸ä¼šä¸ºäº†èº«ä½“çš„æ„‰æ‚¦ å»èˆå¼ƒç²¾ç¥çš„æ„‰æ‚¦|\n",
            "A >:ä¸‡ä¸€ä½ å‘    æ¯”æ¯”æ¯”ç»™è¶£è¶£ä¸ªä½ æˆ‘æˆ‘æˆ‘æˆ‘æˆ‘|\n",
            "sample 5:\n",
            "Q >:ç¡®å®ä¸æ’é™¤è¿™ç§å¯èƒ½æ€§ é‚£å°±éœ€è¦è´£ä»»å¿ƒå»å¹²æ¶‰|\n",
            "A >:è¿™ä¸ªä¹Ÿä¸ä»¬ç°åœ¨çš„ä¸ä¸ä¸ä¸ä¸çš„ä¸çš„ä¸|\n",
            "sample 6:\n",
            "Q >:æ˜¯çš„|\n",
            "A >:æ‚¨ä»Šå¯æ˜¯è´£ä¸Šä¸Šå“ˆå“ˆå“ˆå“ˆä¸å“ˆå“ˆ|\n",
            "-----------------------------------------------------\n",
            "batch 3000 ---------------------------------------\n",
            "  minibatch loss: 2.1847047805786133\n",
            "sample 1:\n",
            "Q >:æ‰€ä»¥äº‹æƒ…å°±è¯´å¾—é€š|\n",
            "A >:æˆ‘å» æˆ‘å¦ˆå¦ˆåœ¨è¯´|\n",
            "sample 2:\n",
            "Q >:æˆ‘ä¹Ÿè§‰å¾—ä½ æŒºä¼˜ç§€çš„ ä¸è¿‡ä½ ä¹Ÿä¸æƒ³è®²ç©¶ å°†å°± æ¯•ç«Ÿé‡åˆ°ä¸€ä¸ªå¾ˆåˆé€‚çš„äººæ¦‚ç‡æ¯”è¾ƒ|\n",
            "A >:å¯¹å•Š æ‰€ä»¥ä½ è¦æˆ‘å‡†å¤‡å˜|\n",
            "sample 3:\n",
            "Q >:æˆ‘è§‰å¾—è¿˜æ˜¯æˆ‘ä»¬ä¸¤ä¸ªåœ¨ä¸€èµ·æ¯”è¾ƒ|\n",
            "A >:ä½ æƒ³æƒ³æ¸…æ¥šå“¦ å“ˆå“ˆå“ˆ å¥½å¥½ å¥½å¥½å¥½|\n",
            "sample 4:\n",
            "Q >:æˆ‘ä¸ä¼šä¸ºäº†èº«ä½“çš„æ„‰æ‚¦ å»èˆå¼ƒç²¾ç¥çš„æ„‰æ‚¦|\n",
            "A >:ä¸‡ä¸€ä½ å‘ç°  æœ‰ä¸ª è¶£è¶£è¶£è¶£ æˆ‘æˆ‘æ›´å¿«æˆ‘æˆ‘|\n",
            "sample 5:\n",
            "Q >:ç¡®å®ä¸æ’é™¤è¿™ç§å¯èƒ½æ€§ é‚£å°±éœ€è¦è´£ä»»å¿ƒå»å¹²æ¶‰|\n",
            "A >:è¿™ä¸ªä¹Ÿä¸ç°ç°åœ¨çš„çš„çš„çš„çš„çš„çš„çƒçƒ|\n",
            "sample 6:\n",
            "Q >:æ˜¯çš„|\n",
            "A >:æ‚¨ä»Šä½ æ˜¯è´£ä»»èƒ½èµ¶ä¹Ÿå“ˆå“ˆå“ˆ|\n",
            "-----------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kBlJnQGR3Fpi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_QTzIo472_AR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7M_9MAGE1W2U",
        "colab_type": "code",
        "outputId": "c7a82976-86b6-493f-fd46-40fd7ed7417b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "cell_type": "code",
      "source": [
        "test_data = []\n",
        "\n",
        "a1 = dict()\n",
        "a1['input'] = [char2idx['å“ˆ'], char2idx['å“ˆ']]\n",
        "a1['output'] = [char2idx['å‘µ'], char2idx['å‘µ']]\n",
        "\n",
        "a3 = dict()\n",
        "a3['input'] = [char2idx['å…°'], char2idx['å¦¹'], char2idx['å¦¹']]\n",
        "a3['output'] = [char2idx['çˆª'], char2idx['å­']]\n",
        "\n",
        "a2 = dict()\n",
        "a2['input'] = [char2idx['æˆ‘'], char2idx['çˆ±'], char2idx['ä½ ']] \n",
        "a2['output'] = [char2idx['å¦ˆ'], char2idx['å–'], char2idx['æ‰¹']]\n",
        "\n",
        "test_data.append(a1)\n",
        "test_data.append(a2)\n",
        "test_data.append(a3)\n",
        "\n",
        "\n",
        "_inputs, _targets, input_lengths, output_lengths = batchFun(test_data) \n",
        "    \n",
        "_batch = {\n",
        "  encoder_inputs: _inputs,\n",
        "  encoder_inputs_length: input_lengths,\n",
        "  decoder_targets: _targets,\n",
        "  decoder_inputs_length: output_lengths,\n",
        "}\n",
        "\n",
        "predict_ = sess.run(decoder_prediction, _batch)\n",
        "#for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
        "\n",
        "for i, (inp,anw, pred) in enumerate(zip(_batch[encoder_inputs].T, _batch[decoder_targets].T, predict_.T)):\n",
        "  print('æ ·æœ¬ {}:'.format(i + 1))\n",
        "  myStr = '' \n",
        "  for j in range(len(inp)):\n",
        "    if inp[j] != 0 and inp[j] != 1:\n",
        "      myStr += idx2char[inp[j]]\n",
        "  print('é—®é¢˜ >:{}|'.format(myStr))\n",
        "  myStr = ''\n",
        "  for j in range(len(anw)):\n",
        "    if anw[j] != 0 and anw[j] != 1:\n",
        "      myStr += idx2char[anw[j]]\n",
        "  print('æ­£ç¡®ç­”æ¡ˆ >:{}|'.format(myStr))\n",
        "  myStr = ''\n",
        "  for j in range(len(pred)):\n",
        "    if pred[j] != 0 and pred[j] != 1:\n",
        "      myStr += idx2char[pred[j]]\n",
        "  print('é¢„æµ‹ >:{}|'.format(myStr))\n",
        "  if i >= 20:\n",
        "    break\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "æ ·æœ¬ 1:\n",
            "é—®é¢˜ >:å“ˆå“ˆ|\n",
            "æ­£ç¡®ç­”æ¡ˆ >:å‘µå‘µ|\n",
            "é¢„æµ‹ >:  å‘|\n",
            "æ ·æœ¬ 2:\n",
            "é—®é¢˜ >:æˆ‘çˆ±ä½ |\n",
            "æ­£ç¡®ç­”æ¡ˆ >:å¦ˆå–æ‰¹|\n",
            "é¢„æµ‹ >:å“¼æˆ‘æœ›|\n",
            "æ ·æœ¬ 3:\n",
            "é—®é¢˜ >:å…°å¦¹å¦¹|\n",
            "æ­£ç¡®ç­”æ¡ˆ >:çˆªå­|\n",
            "é¢„æµ‹ >:ç”µæ—¥|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wwsrpOik5zhv",
        "outputId": "ab4a2483-997f-4bf7-d9ec-fef7cadf359b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss_track)\n",
        "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 2.1842 after Tensor(\"mul:0\", shape=(), dtype=int32) examples (batch_size=Tensor(\"unstack:1\", shape=(), dtype=int32))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWkAAAD4CAYAAAAuNhccAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrU8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "KapbnDDMw06z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}